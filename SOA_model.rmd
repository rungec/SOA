---
title: "State of Arctic models"
output: html_document
---
  
```{r packages and data, include=FALSE, cache=T}
require(knitr)
require(tidyverse)
require(lubridate)
require(itsadug) #for gam visualisation
require(mgcv) #for gam mixed effects models
#require(broom) #for nested modelling
require(modelr)
require(gridExtra) #for grid.arrange
require(ggfortify) #for autoplot
require(pander) #for pander table
require(ggpubr) #for ggarrange

#set directories
wd = "D:/Box Sync/Arctic/CONNECT/Paper_5_state_of_arctic/Analysis/Intermediate"

#Industries
industries <- c("Cruise_tourism", "Domestic_tourism", "International_tourism", "Population", "Shipping_distance", "Hunters", "Fishing", "Oilandgas_wells")

#a function to change the plot sizes
subchunkify <- function(g, fig_height=7, fig_width=5) {
  g_deparsed <- paste0(deparse(
    function() {g}
  ), collapse = '')
  
  sub_chunk <- paste0("
  `","``{r sub_chunk_", floor(runif(1) * 10000), ", fig.height=", fig_height, ", fig.width=", fig_width, ", echo=FALSE}",
  "\n(", 
    g_deparsed
    , ")()",
  "\n`","``
  ")
  
  cat(knitr::knit(text = knitr::knit_expand(text = sub_chunk), quiet = TRUE))
}

```

```{r merge data, include=FALSE, cache=T, dependson="packages and data"}
#load and merge files
alldat <- lapply(industries, function(i) {
  print(paste0("loading ", i))
    
    dat <- read.csv(paste0(wd, "/", i, "_long.csv"), header=TRUE, fileEncoding = "UTF-8-BOM", stringsAsFactors = FALSE)
    dat$value_raw <- dat$value
    dat$subIndustry <- i
    if(i %in% c("Cruise_tourism", "Domestic_tourism", "International_tourism")){
        dat$Industry <- "Tourism"
    } else {
        dat$Industry <- i
    }
    
      if(i == "Shipping_distance"){ 
        dat <- dat %>% filter(Country!="Total") %>% 
                  group_by(Country, Region, Industry, subIndustry, year) %>% 
                  summarise(value=sum(value, na.rm=TRUE)) %>% #sum shipping distance across all types of shipping
                  mutate(Metric="nautical miles sailed all shipcats") %>% 
                  mutate(value_raw=value)
        dat$value[dat$value==0] <- 1 #add small constant to make growth rate calculation possible
        
      } 
    
      if(i == "Fishing"){ 
        dat <- dat %>% filter(Region!="Total") 
      } 
      

      if(i == "Cruise_tourism"){ 
      #Murmansk cruise tourism statistics use a different metric than elsewhere, number of ship calls as opposed to passengers.
      #We convert ship calls to an estimation of passenger numbers by multiplying the number of calls by the average vessel capacity of ships calling in 2016 (437.4 passengers).
      #adjust murmansk
      dat$value[dat$Region=="Murmansk"] <-  dat$value[dat$Region=="Murmansk"]*437.4
      dat[dat$Region=="Murmansk", "Metric"] <- "Passengers"
      #drop other metrics
      dat <- dat %>% filter(Metric=="Passengers")
      } 
      
      if(i == "Hunters") { 
        #drop other types of hunting
        dat <- dat %>% filter(Metric %in% c("All hunting/trapping/fishing licenses", "Number of hunters who paid game management fees", "Registered hunters", "Paid hunting permits", "Sports hunters"))
        #drop extra column
        dat <- dat[, which(!names(dat)=="Region_original")]
      }
      
      if(i == "Oilandgas_wells") {
        #remove duplicated data: drop Region="all" - this is a sum of onshore and offshore; and purpose="all" - this is sum of exploration & development
        dat <- dat %>% filter(Region!="All" & Purpose!="All"|is.na(Purpose) & Metric!="All") %>% #have to explicitly select NAs to get Alaska etc
              group_by(Country, Region, Metric, year) %>% #the metric column tells us whether exploration or dev
              summarise(value_raw=sum(value_raw, na.rm=TRUE)) %>% #sum the different types of wells (injection, wildcat etc)
              mutate(subIndustry=paste("Oilandgas", Metric, sep="_")) #split Industry into exploration and development      
              dat$Metric <- "n_wells"
              dat$Industry <- "OilandGas"

        #change value to cumsum
        dat <- dat %>% group_by(Country,Region,subIndustry) %>% 
                arrange(year) %>%
                mutate(value=cumsum(value_raw)) %>% #cumulative sum of n wells
                mutate(value=replace(value, value==0, NA)) #replace zeros with NA
      }
      
    return(data.frame(dat))

})
alldat <- bind_rows(alldat)
#alldat$value[alldat$value==0] <- NA #replace zeros with NA, otherwise anngrowthfun returns Inf
alldat$Country[alldat$Country %in% c("Svalbard & Jan Mayen", "Svalbard and Jan Mayen")] <- "Svalbard"
alldat <- alldat %>% mutate_at(c("Country", "Region", "Industry", "subIndustry"), funs(factor)) %>% #convert to factors
                    filter(!is.na(value)) #drop NAs
write_excel_csv(alldat, "All_industries_long.csv") #for fileEncoding = "UTF-8-BOM"

```

### Intensity of activities
We calculate the intensity of each industry using the mean value across 2012-2017.  
we use a spread of years rather than a single year to account for interannual variation and missing data.

```{r intensities, include=FALSE, cache=T, dependson=c("packages and data", "merge data")} 

intensitydf <- alldat %>%  filter(!is.na(value) & !Industry %in% c("OilandGas")) %>% #drop years missing data
  filter(year %in% 2012:2017) %>% 
  group_by(Country, Region, Industry, subIndustry, Metric)  %>%
  summarise(intensity=mean(value)) 
```

We calculate the intensity for oil and gas diffently, as the total number of wells drilled between 1960 and 2017.

```{r fixintensity1, include=FALSE, cache=T, dependson=c("packages and data", "merge data")}
oilintensity <- alldat %>%  filter(!Industry %in% c("OilandGas")) %>% 
  filter(year %in% 1960:2017) %>% 
  group_by(Country, Industry, subIndustry, Region, Metric)  %>%
  summarise(intensity=sum(value_raw, na.rm=TRUE))

#replace oil and gas values with these new values
intensitydf <- bind_rows(intensitydf, oilintensity)
intensitydf <- intensitydf %>% drop_na() %>% droplevels()
write_excel_csv(intensitydf, "All_industries_intensity.csv") #for fileEncoding = "UTF-8-BOM"

```

Murmansk cruise tourism statistics use a different metric than elsewhere, number of ship calls as opposed to passengers.
We convert ship calls to an estimation of passenger numbers by multiplying the number of calls by the average vessel capacity of ships calling in 2016 (437.4 passengers). This makes no difference to the zscore, but puts the intensity on the same scale as other regions. This gives higher than expected cruise passengers - same intensity as Troms...

```{r, include=FALSE, cache=T, dependson=c("packages and data", "merge data")}
alldat %>% filter(subIndustry=="Cruise_tourism" & Region %in% c("Murmansk", "Troms", "Finnmark", "Arkhangel'sk") & year==2016) %>% select(Region, value)
```

The plots for Domestic and International tourism and Hunters should be interpreted with caution - different countries use different metrics.  

#### Domestic & International tourism
Canada, Alaska, Russia = number of visitors    
All other countries = guest nights 

#### Hunters
Alaska = All hunting/trapping/fishing licenses  
Greenland = Sports hunters,  
Norway = Registered hunters,  
Finland & Sweden = Paid hunting permits,  
No data for Russia, Faroes, Iceland, Canada.  

```{r plot intensities, echo=FALSE, warning=FALSE, include=FALSE, cache=T, dependson=c("packages and data", "merge data", "intensities", "fixintensity1")}
intensitydf %>% filter(Industry=="Tourism") %>%
ggplot(aes(x=Country, y=intensity)) +
  geom_col() +
  facet_wrap(~ subIndustry, scales="free_y") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5))

intensitydf %>% filter(!Industry %in% c("Tourism", "Fishing", "Shipping_distance")) %>%
ggplot(aes(x=Country, y=intensity)) +
  geom_col() +
  facet_wrap(~ subIndustry, scales="free_y") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5))

intensitydf %>% filter(Industry=="Fishing") %>%
ggplot(aes(x=Region, y=intensity)) +
  geom_col() +
  ggtitle("Fishing catch") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5))

intensitydf %>% filter(Industry=="Shipping_distance") %>%
ggplot(aes(x=Region, y=intensity)) +
  geom_col() +
  ggtitle("Shipping distance") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5))

```  

 
### Data processing:
#### Shipping distance: 
In some parts of the Arctic there is no shipping in certain years (e.g. Davis Sea in 2012). I added a small constant (1) to these numbers, because log10(0) evaluates to -Inf, which can't be modelled.

#### Oil and gas:
We calculate annual activity in oil and gas from the cumulative number of wells drilled across time, as activity is intermittent (lots of years with no wells drilled). This gives growth rates of zero in years with no activity, rather than growth rates of Inf. This can be loosely taken as a measure of the growth (both rise and fall) in the industry across time.

#### Cruise tourism
Murmansk values adjustment described above makes no difference to the znorm.

#### Normalisation
Since different metrics are used within an industry across the Arctic (e.g. guest nights vs number of visitors) we z-score standardise the values (subtract mean of all datapoints for an individual point, then divide by standard deviation of all points). In order to have robust estimates, we dropped regions where we could only find data for 3 or fewer years (e.g. cruise tourism in Sakha, n=2).

#### Interpolation
Note that we don't include years with no data in the model i.e. perform no interpolation for missing data.

```{r znorm, include=FALSE, cache=T, dependson=c("packages and data", "merge data")}
znormdat <- alldat %>%  filter(!is.na(value)) %>% #drop years missing data
  group_by(subIndustry, Region) %>% 
  mutate(znorm = scale(value, center=TRUE, scale=TRUE) %>% as.vector)

#check that the data transformation we do in chunk 'merge data' hasn't dramatically changed the zscores
# alldat2 <- alldat
# alldat2[alldat2$Industry=="OilandGas", "value_raw"] <- alldat2[alldat2$Industry=="OilandGas", "value"]
# znormdatcheck <- alldat2 %>%  filter(!is.na(value)) %>% #drop years missing data
#   group_by(subIndustry, Region) %>% 
#   mutate(znorm = scale(value_raw, center=TRUE, scale=TRUE) %>% as.vector)
# plot(znormdat$znorm, znormdatcheck$znorm)

#how many values are there for each region?
n <- alldat %>% filter(!is.na(value)) %>% 
  group_by(subIndustry, Region) %>% 
  count()
#write.table(n, "SOA_nobservations_byregion_andindustry.csv")

#drop regions with insuffient data to calculate robust znorm
dropregions <- n[n$n <= 3, ] #drop with 3 or fewer observations
znormdat <- znormdat[!(znormdat$Region %in% dropregions$Region & znormdat$subIndustry %in% dropregions$subIndustry), ]

#write_excel_csv(znormdat, "All_industries_znorm.csv") #for fileEncoding = "UTF-8-BOM"

```

### Models using 'GAM' function
We test 2 models:  
#### Model 1
The full gamm model of z-normalised value ~ smoothed year with Region as a random effect (random slope and intercept), and with an autocorrelation AR term with lag=1. We look at the ACF plots and Ljung-Box test to see if there is any significant temporal autocorrelation (remembering that by chance we expect 1 out of every 20 sample autocorrelations to exceed the 95% bounds (the blue dotted lines)) 

#### Model 2
We then add in the autocorrelation term, and see if the model performs better. 

```{r modelsgam, include=TRUE, warning=FALSE, cache=T, dependson=c("packages and data", "merge data", "modelplotfuns", "znorm")}
################
#Main processing
################
industries <- unique(znormdat$subIndustry)
industries <- industries[industries!="Population"]
i <- industries[1]
#for(i in industries){

currdf <- znormdat %>% filter(subIndustry==i) %>% droplevels()
print(as.character(i))


#https://stats.stackexchange.com/questions/335883/specifying-generalized-additive-models-in-r-using-the-mgcv-package
#https://cran.r-project.org/web/packages/itsadug/vignettes/test.html

mod1 <- gam(znorm ~ s(year) + s(year, Region, bs='fs'), data=currdf)
gamtabs(mod1, type="HTML")

print(summary(mod1))
#diagnostic plots
par(mfrow=c(2,3), cex=1.1)
diagnostics(m1, ask=FALSE)

#Print the ACF of the model by region
acf_resid(m1, split_pred = c("Region"), n=length(levels(currdf$Region)))
#is autocorrelation significant?
print(Box.test(resid(mod1), lag=20, type="Ljung-Box"))

#plot the partial effects
par(mfrow=c(1,2), cex=1.1)
plot(m1, select=1, shade=TRUE, scale=0, main=i) #first smooth
abline(h=0)
plot(m1, select=2, shade=TRUE, scale=0) #second smooth
abline(h=0)

for(x in unique(currdf$Region)){
#Plot smooths by region
  par(mfrow=c(1,2), cex=1.1)
  # including random effects:
  plot_smooth(m1, view="year", cond=list(Region=x), main=i)
  # excluding random effects:
  plot_smooth(m1, view="year", cond=list(Region=x), rm.ranef=TRUE, main=x)
}
#plot data
plot_data(m1, view="year", split_by="Region", cex=.5)
inspect_random(m1, select=2)

#choose the rho value of the temporal autocorrelation
rhoval <- acf(resid(mod1), plot=FALSE)$acf[2]
#model with autocorrelation
mod2 <- gam(znorm ~ s(year) + s(year, Region, bs='fs'), rho=rhoval, data=currdf)
print(summary(mod2))
print(compareML(mod1, mod2))
par(mfrow=c(1,2))
acf(resid(mod2), lag=20, main="ACF Model 2")
pacf(resid(mod2), lag=20, main="PACF Model 2")

```

### Models using 'GAMM' function
We test 4 models:  
#### Model 1
A gam model of z-normalised value ~ smoothed year. We look at the ACF plots and Ljung-Box test to see if there is any significant temporal autocorrelation (remembering that by chance we expect 1 out of every 20 sample autocorrelations to exceed the 95% bounds (the blue dotted lines)) 
#### Model 2
We then add in the autocorrelation term, and see if the model performs better. We compare two correlation structures
mod2a = model with autoregressive process of order 1
mod2b = model with autoregressive moving average process of order 1
mod2c = model with autoregressive moving average process of order 1

#### Model 3
mod3. a gamm model of z-normalised value ~ smoothed year with Region as a random effect, and look for autocorrelation.  

#### Model 4
mod4. the full gamm model of z-normalised value ~ smoothed year with Region as a random effect, and with an autocorrelation term with lag=1.


```{r modelplotfuns, include=FALSE, warning=FALSE, cache=T, dependson=c("packages and data", "merge data")}
#znormdat <- read.csv("All_industries_znorm.csv", header=TRUE, fileEncoding = "UTF-8-BOM")

#set up function to plot the model outputs
acfplotfun <- function(currmod){
  #plot the model autocorrelation and acf plot
  par(mfrow=c(1,2))
  acf(resid(currmod$lme, type="normalized"), lag=20)
  pacf(resid(currmod$lme, type="normalized"), lag=20)
}

comparemodelsplot <- function(m1, m2){
    #set up predictions  
    grid <- currdf %>% 
      data_grid(year, Region) %>%
      add_predictions(m1)
    grid2 <- currdf %>% 
      data_grid(year, Region) %>%
      add_predictions(m2)
  #plot predictions
  ggplot(currdf, aes(year, znorm, color=Region)) +
    geom_point() +
    geom_line(data=grid, aes(x=year, y=pred), col="black", inherit.aes = FALSE) +
    geom_line(data=grid2, aes(x=year, y=pred), col="blue", inherit.aes = FALSE) +
    annotate("text", 
             x=(min(currdf$year)+4), 
             y=c(max(currdf$znorm)-0.1, max(currdf$znorm)-0.4), 
             col=c("black", "blue"), size=4, hjust=0, label=c("Uncorrelated Errors","AR(1) Errors")) +
    geom_segment(x=(min(currdf$year)+1), xend=(min(currdf$year)+3), 
             y=max(currdf$znorm)-0.1, yend=max(currdf$znorm)-0.1, 
             col="black", inherit.aes = FALSE) +
    geom_segment(x=(min(currdf$year)+1), xend=(min(currdf$year)+3), 
             y=max(currdf$znorm)-0.4, yend=max(currdf$znorm)-0.4, 
             col="blue", inherit.aes = FALSE) +
    theme_minimal()
}
  
plotmodelbyregion <- function(m1){
  #set up data
  pred <- predict(m1, se.fit=T)
  #plot 
  currdf %>% ggplot(aes(year, znorm)) +
    geom_point() +
    geom_ribbon(aes(ymin=pred$fit-1.96*pred$se.fit, ymax=pred$fit+1.96*pred$se.fit), alpha=0.2, fill="grey30")+
    geom_line(aes(y=pred$fit), col="black", lwd=1) +
    facet_wrap(~Region) +
    theme(legend.position="none") 
 } 
  
 
```


```{r models_gamm, include=FALSE, warning=FALSE, cache=T, dependson=c("packages and data", "merge data", "modelplotfuns", "znorm")}
################
#Main processing
################
industries <- unique(znormdat$subIndustry)
industries <- industries[industries!="Population"]
i <- industries[1]
#for(i in industries){

currdf <- znormdat %>% filter(subIndustry==i) %>% droplevels()
print(as.character(i))

#Basic model, ignoring temporal autocorrelation
mod1 <- gamm(znorm ~ s(year), random=NULL, correlation=NULL, data=currdf)
acfplotfun(mod1) #examine the autocorrelation
# https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/ for a good discussion of how to interpret these plots, and whether there is AR or MA correlation
## if it seems like like sharp cut-off in ACF and PACF - AR terms probably best
## if it seems like ACF drops off slowly - ARMA terms probably best
print(Box.test(resid(mod1$lme), lag=20, type="Ljung-Box")) #do we have any significant temporal autocorrelation?

#Add in temporal autocorrelation with 1yr lag
mod2a <- gamm(znorm ~ s(year), correlation = corAR1(form = ~ year | Region), data=currdf)
mod2b <- gamm(znorm ~ s(year), correlation = corARMA(form = ~ year | Region, p=1), data=currdf) #1yr lag
mod2c <- gamm(znorm ~ s(year), correlation = corARMA(form = ~ year | Region, p=2), data=currdf) #2yr lag
print(anova(mod2a$lme, mod2b$lme, mod2c$lme)) #any difference?
print(anova(mod2a$lme, mod1$lme)) #any difference?
acfplotfun(mod2a) #any correlation remaining?

#compare mod1 and mod2
comparemodelsplot(mod2a$gam, mod1$gam)
plot(mod2a$gam, residuals = TRUE, pch = 19, cex = 0.75)
#Fitted thin-plate spline with AR(1) residuals and approximate 95% point-wise confidence interval

#add a random intercept
mod3 <- gamm(znorm ~ s(year), random=list(Region=~1), correlation = corAR1(form = ~ year | Region), data=currdf)
plot(mod3$gam, residuals = TRUE, pch = 19, cex = 0.75)
print(summary(mod3$lme))
print(summary(mod3$gam))
print(summary(mod3$lme))
print(anova(mod3$gam) )

#add a random correlated slope and intercept
mod4 <- gamm(znorm ~ s(year), random=list(Region=~1, Region=~year), correlation = corAR1(form = ~ year | Region), data=currdf)
plot(mod4$gam, residuals = TRUE, pch = 19, cex = 0.75)
print(summary(mod4$gam))
print(summary(mod4$lme))
print(anova(mod4$lme) )
plotmodelbyregion(mod4$gam)
comparemodelsplot(mod3$gam, mod4$gam)
print(anova(mod4$lme, mod1$lme))

#NEXT STEPS
# adjust k value and bs, and test method=REML
# use this methods https://www.fromthebottomoftheheap.net/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/ to see if changes are significant
#figure out how to add error bars


```
